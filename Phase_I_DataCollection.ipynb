{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase I: Data Collection\n",
    "\n",
    "#### Choosing the Classes: r/Science and r/EverythingScience\n",
    "\n",
    "This analysis will look at the similarities and differences in the text data of two distinct subreddits: r/Science and r/EverythingScience. I chose these two subreddits because they are alike in topic and in post format (i.e. users typically post summaries of linked articles or reserach studies). The only place where the subreddits differ is that r/Science requires posts to be peer-reviewed studies, whereas r/EverythingScience does not. Initially, I had chosen r/Psychology as the counterpart to r/EverythingScience, out of a concern that using two subreddits on the exact same topic might cause poor model performance. However, given that the objective is to test the predictive power of models to judge the validity of a claim based solely on text, I decided to keep all of the variables in the experiment as fair as possible. If the models can't distinguish between the posts of each science subreddit, in that case we can change a variable like subreddit topic and measure any changes in model performance.\n",
    "\n",
    "\n",
    "#### Data Collection Methodology\n",
    "\n",
    "In order to gather each subreddit's data and convert it into a usable format, I queried the Reddit API and requested data in the form of a JSON string. Reddit allows 25 posts to be returned from a single API request, so I implemented a for loop in order to accumulate enough data points from each subreddit. I appended this JSON data to a list, and was able to create a DataFrame from there. I repeated this process a few times over multiple days - both to generate enough data, and avoid querying the API too aggressively - and thus the following code only represents one cycle through the API query / data collection process. Ultimately I wound up with three separate .csv files of data that I later merged into a singe DataFrame. Since the initial API queries involved the r/Psychology subreddit, which I eventually excluded form this analysis, I performed some filtering on the data set before exporting the final .csv at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Importing Packages & Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the API and collecting post data to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes on the for loop:\n",
    " \n",
    "- In order to deal with pagination, this for loop needed to have some sort of reference so that it could keep iterating through successive pages and returning consecutive, unique posts. The \"after\" variable is the unique post ID of the last post on each page.\n",
    "- This partucular API employs a firewall, so I used a user-agent to bypass any potential errors here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = None\n",
    "headers = {'User-agent': 'bot 0.1'}\n",
    "urls = ['https://www.reddit.com/r/science/new/.json', 'https://www.reddit.com/r/EverythingScience.json', \n",
    "        'https://www.reddit.com/r/psychology/new/.json']\n",
    "\n",
    "for url in urls:\n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        print(after)\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "            after = the_json['data']['after']\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran this loop multiple times to see how many posts I could accumulate in a single day, and then checked my list of posts to confirm the number of unique entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1402"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1227"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(p['data']['name'] for p in posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame from the relevant post data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1227, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([p['data'] for p in posts]).drop_duplicates(subset='name')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/redditAPI_json12202018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all .csv files of collected data into one data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('./Datasets/redditAPI_json12132018.csv')\n",
    "csv_2 = pd.read_csv('./Datasets/redditAPI_json12172018.csv')\n",
    "csv_3 = pd.read_csv('./Datasets/redditAPI_json12202018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>dongasaurus_prime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>100% renewables, No problems: \"contrary to uns...</td>\n",
       "      <td>34</td>\n",
       "      <td>https://physicsworld.com/a/100-renewables-no-p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>twwitterr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Scientists Discovered Rare Giant Viruses Lurki...</td>\n",
       "      <td>20340</td>\n",
       "      <td>https://www.google.com/amp/s/www.sciencealert....</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>BloodSoakedDoilies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Planckian Dissipation, Strange Metals, and a Q...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.theatlantic.com/science/archive/20...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Wagamaga</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reward1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Eating leafy greens, dark orange and red veget...</td>\n",
       "      <td>38</td>\n",
       "      <td>https://eurekalert.org/pub_releases/2018-11/aa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>FillsYourNiche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>env   reward3</td>\n",
       "      <td>[{'e': 'text', 't': 'MS | Ecology and Evolutio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MS | Ecology and Evolution | Ethology</td>\n",
       "      <td>...</td>\n",
       "      <td>136.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Breast tumors can boost their growth by recrui...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.eurekalert.org/pub_releases/2018-1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      approved_at_utc  approved_by  archived              author  \\\n",
       "1222              NaN          NaN     False   dongasaurus_prime   \n",
       "1223              NaN          NaN     False           twwitterr   \n",
       "1224              NaN          NaN     False  BloodSoakedDoilies   \n",
       "1225              NaN          NaN     False            Wagamaga   \n",
       "1226              NaN          NaN     False      FillsYourNiche   \n",
       "\n",
       "     author_cakeday  author_flair_background_color author_flair_css_class  \\\n",
       "1222            NaN                            NaN                    NaN   \n",
       "1223            NaN                            NaN                    NaN   \n",
       "1224            NaN                            NaN                    NaN   \n",
       "1225            NaN                            NaN                reward1   \n",
       "1226            NaN                            NaN          env   reward3   \n",
       "\n",
       "                                  author_flair_richtext  \\\n",
       "1222                                                 []   \n",
       "1223                                                 []   \n",
       "1224                                                 []   \n",
       "1225                                                 []   \n",
       "1226  [{'e': 'text', 't': 'MS | Ecology and Evolutio...   \n",
       "\n",
       "      author_flair_template_id                      author_flair_text ...   \\\n",
       "1222                       NaN                                    NaN ...    \n",
       "1223                       NaN                                    NaN ...    \n",
       "1224                       NaN                                    NaN ...    \n",
       "1225                       NaN                                    NaN ...    \n",
       "1226                       NaN  MS | Ecology and Evolution | Ethology ...    \n",
       "\n",
       "     thumbnail_height thumbnail_width  \\\n",
       "1222             78.0           140.0   \n",
       "1223             56.0           140.0   \n",
       "1224             72.0           140.0   \n",
       "1225             46.0           140.0   \n",
       "1226            136.0           140.0   \n",
       "\n",
       "                                                  title    ups  \\\n",
       "1222  100% renewables, No problems: \"contrary to uns...     34   \n",
       "1223  Scientists Discovered Rare Giant Viruses Lurki...  20340   \n",
       "1224  Planckian Dissipation, Strange Metals, and a Q...     13   \n",
       "1225  Eating leafy greens, dark orange and red veget...     38   \n",
       "1226  Breast tumors can boost their growth by recrui...     13   \n",
       "\n",
       "                                                    url  user_reports  \\\n",
       "1222  https://physicsworld.com/a/100-renewables-no-p...            []   \n",
       "1223  https://www.google.com/amp/s/www.sciencealert....            []   \n",
       "1224  https://www.theatlantic.com/science/archive/20...            []   \n",
       "1225  https://eurekalert.org/pub_releases/2018-11/aa...            []   \n",
       "1226  https://www.eurekalert.org/pub_releases/2018-1...            []   \n",
       "\n",
       "      view_count  visited  whitelist_status  wls  \n",
       "1222         NaN    False           all_ads    6  \n",
       "1223         NaN    False           all_ads    6  \n",
       "1224         NaN    False           all_ads    6  \n",
       "1225         NaN    False           all_ads    6  \n",
       "1226         NaN    False           all_ads    6  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvs = [csv_1, csv_2, csv_3]\n",
    "df = pd.concat(csvs)\n",
    "df.drop_duplicates(subset='name', inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2832, 102)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EverythingScience', 'psychology', 'science'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EverythingScience', 'science'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['subreddit'] != 'psychology']\n",
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1916, 102)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / exporting the final data set for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./Datasets/Final_Reddit_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
